{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIBEva6M_CHx"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import time\n",
        "\n",
        "# Инициализация SparkSession\n",
        "#spark = SparkSession.builder.appName(\"SparkApp\").getOrCreate()\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SparkApp\") \\\n",
        "    .config(\"spark.driver.extraJavaOptions\", \"-D java.security.manager=allow\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Запись времени начала выполнения\n",
        "start_time = time.time()\n",
        "\n",
        "# Загрузка данных\n",
        "spark_data = spark.read.csv('data.csv', header=True, inferSchema=True)\n",
        "\n",
        "# Обработка данных\n",
        "result = spark_data.groupBy('column').agg({'value': 'sum'}).collect()\n",
        "\n",
        "# Время выполнения агрегации\n",
        "agg_time = time.time() - start_time\n",
        "\n",
        "# Создание двумерных матриц 10000x10000 в виде DataFrame\n",
        "rows1 = [(i, j, float(i + j)) for i in range(10000) for j in range(10000)]\n",
        "rows2 = [(i, j, float(i * j)) for i in range(10000) for j in range(10000)]\n",
        "\n",
        "matrix1 = spark.createDataFrame(rows1, [\"row\", \"col\", \"value\"])\n",
        "matrix2 = spark.createDataFrame(rows2, [\"row\", \"col\", \"value\"])\n",
        "\n",
        "# Замер времени выполнения для транспонирования\n",
        "transpose_start_time = time.time()\n",
        "transposed_matrix1 = matrix1.groupBy(\"col\").pivot(\"row\").agg({\"value\": \"first\"})  # Транспонирование\n",
        "transpose_time = time.time() - transpose_start_time\n",
        "\n",
        "# Замер времени выполнения для перемножения матриц\n",
        "multiply_start_time = time.time()\n",
        "\n",
        "# Получение транспонированной матрицы как RDD для умножения\n",
        "transposed_rdd = transposed_matrix1.rdd.map(lambda row: row.asDict()).collect()\n",
        "matrix1_as_dict = {row['row']: row for row in transposed_rdd}\n",
        "\n",
        "# Функция для умножения матриц\n",
        "def matrix_multiplication(row):\n",
        "    result = []\n",
        "    for col in range(10000):\n",
        "        sum_product = sum(matrix1_as_dict.get(r, {}).get(col, 0) * matrix2.filter(matrix2.row == r).filter(matrix2.col == col).select(\"value\").first()[0]\n",
        "                          for r in range(10000))\n",
        "        result.append((row[0], col, sum_product))\n",
        "    return result\n",
        "\n",
        "product_rdd = matrix1.rdd.map(matrix_multiplication).flatMap(lambda x: x)\n",
        "\n",
        "# Перевод результата обратно в DataFrame\n",
        "product_matrix = product_rdd.toDF([\"row\", \"col\", \"value\"])\n",
        "\n",
        "multiply_time = time.time() - multiply_start_time\n",
        "\n",
        "# Общее время выполнения\n",
        "total_execution_time = time.time() - start_time\n",
        "\n",
        "# Вывод результата\n",
        "print(\"Результат Spark:\")\n",
        "for row in result:\n",
        "    print(row)\n",
        "\n",
        "print(f\"Время агрегации: {agg_time:.6f} секунды\")\n",
        "print(f\"Время транспонирования: {transpose_time:.6f} секунды\")\n",
        "print(f\"Время умножения: {multiply_time:.6f} секунды\")\n",
        "print(f\"Общее время выполнения: {total_execution_time:.6f} секунды\")\n",
        "\n",
        "# Вывод размеров полученной матрицы\n",
        "print(f\"Количество в результирующей матрице: {product_matrix.count()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnrEZLnYDMBy",
        "outputId": "4d49a64c-9208-48e9-e3c5-4938f9fe89b2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['column', 'value'], dtype='object')\n",
            "Dask Result:\n",
            "           value\n",
            "column          \n",
            "10      60611700\n",
            "11      60553503\n",
            "12      60596085\n",
            "13      60678129\n",
            "14      60582142\n",
            "...          ...\n",
            "95      60560084\n",
            "96      60564514\n",
            "97      60506660\n",
            "98      60530565\n",
            "99      60506282\n",
            "\n",
            "[90 rows x 1 columns]\n",
            "Agg Time: 16.955524 seconds\n",
            "Matr Time: 0.049069 seconds\n",
            "Total Execution Time: 17.004593 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/dask/array/routines.py:332: PerformanceWarning: Increasing number of chunks by factor of 10\n",
            "  intermediate = blockwise(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iy3QHQR0_6ZB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}